---
layout: post
title: deep learning ch05.machine learning basics
date: 2016-04-25 12:00:00
---
> 本博文是Ian Goodfellow, Yoshua Bengio and Aaron Courville写的deep learning的个人读书笔记  
> 原书地址：[deep learning](http://www.deeplearningbook.org/)  
> 第五章地址：[machine learning basics](http://www.deeplearningbook.org/contents/ml.html)

## **5.2，复杂度、过拟合和欠拟合**

机器学习有别于优化问题的地方在于，优化问题是直接在已知数据上优化，机器学习是在已知数据上（训练数据）优化未知数据（测试数据）的效果

之所以能在已知数据上优化未知数据的效果，是由于建立在测试数据和训练数据同分布的假设上的

机器学习算法的复杂度（capacity）不一样，会导致模型过拟合或者欠拟合

有多种方式可以改变机器学习算法的复杂度（capacity）

- 表示复杂度：模型的选择
- 有效复杂度：优化函数的选择

### **5.2.1，没有免费的午餐理论**

> 没有银弹。即没有在所有领域、所有数据集上都表现很好的机器学习算法

### **5.2.2，正则化**

使用正则化方法在`拟合训练数据`和`模型复杂度`中取一个折中

## **5.3，超参和交叉验证**

超参：模型学习中我们可以改变的值，比如学习率

超参一般都是人为确定而不是通过学习得到的，因为如果通过学习得到的话，模型总倾向于得到一个复杂度最高的模型，这样会导致**过拟合**

为了解决自己手动调参带来的过拟合问题，可以引入**交叉验证**的方法

## **5.4，估计、偏差（bias）和方差**

数学知识

## **5.5，最大似然估计**

最大似然估计是为不同的模型派生好的估计函数的准则，即理论支撑

最大似然的通俗解释：若一个分布是未知的，但是知道从这个分布中选出的N个样本，则使得这N个样本被选出概率最大的分布即为要估计的未知分布

似然函数可以通过两边取log化乘为加

解释最大似然的一种方法是通过相对熵（KL divergence）

### **5.5.1，条件log最大似然以及均方误差**

> 均方误差函数可以看成高斯噪声模型的假设下的最大似然解。

### **5.5.2，最大似然的特性**

在一定的条件下，最大似然有**一致性**的特性：随着数据量增大，最大似然估计的结果是向真实结果收敛的。

- 真实的分布是已知的传统分布
- 分布是由一个参数决定的

一致性带来的好处体现在Statistical efficiency上，即可以用更少的数据取得一样的泛化错误率，等效的说是可以用一样的数据取得更小的泛化错误率

## **5.6，贝叶斯统计**

最大似然函数属于频率主义学派，下面要讲的是贝叶斯学派

频率主义学派觉得参数$\theta$是固定值但是未知，贝叶斯学派觉得参数$\theta$是随机变量且符合某种分布

### **5.6.1，最大后验概率估计**

$$ \theta_{MAP} = \mathop{\arg\,\max}\limits_\theta p(\theta|x) = \mathop{\arg\,\max}\limits_\theta \log p(x|\theta) + \log p(\theta) $$

## **5.7，有监督学习算法**

有监督学习即知道label

### **5.7.1，概率有监督学习**

本书中大多数的有监督学习算法是基于估计$p(y|x)$，可以用最大似然估计去求出使得$p(y|x;\theta)$最好的参数$\theta$
